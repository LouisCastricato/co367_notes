Non-linear optimization

Prof. Laurent Poirrier

Texts:
Nocedal/Wright: Numerical Optimization
Boyd/Vandenberghe: Convex Optimization

=LEC1= 2018-09-07
_CH0-Introduction_

 Mathematical Optimization or Mathematic Programming
  Informally: Find a best solution to the model of a problem
   *best* according to a given objective/criterion
 
 Applications
  Operations Research
   Scheduling + Planning
   Supply Chain Management
   Vehicular Routing
   Power Grid Optimization
  Statistics & Machine Learning
   Curve Fitting
   Classification, Clustering, SVM, ...
   Deep Learning
  Finance
  Optimal Control
  Biology
 
 
 (OPT) min f(x)
  s.t. g_i(x) ≤ 0, for i ∈ {1, 2, 3, ..., m}
  x ∈ R
 
 Remarks:
  a. max f(x) = -(min -f(x))
  b. {x ∈ Rⁿ: g(x) ≥ 0} = {x ∈ Rⁿ: -g(x) ≤ 0}
  c. {x ∈ Rⁿ: g(x) ≥ b} = {x ∈ Rⁿ: -g(x) - b ≤ 0}
 
 A. Classification of Solutions
  defn Open Ball:
   the open ball of radius δ around x_bar is B_δ(x_bar) = {x ∈ Rⁿ: ‖x - x_bar‖ ≤ δ}
 
  defn Minimizer:
   Consider f: D → R. The point x* ∈ D is
    a global minimizer for f on D if 
     f(x*) ≤ f(x), ∀x ∈ D
 
    a strict global minimizer for f on D if 
     f(x*) < f(x), ∀x ∈ D, x ≠ x*
    
    a local minimizer for f on D if
     ∃δ > 0: f(x*) ≤ f(x), ∀x ∈ B_δ(x*) ∩ D
 
    a strict local minimizer for f on D if
     ∃δ > 0: f(x*) < f(x), ∀x ∈ B_δ(x*) ∩ D, x ≠ x*
 
 B. Classification of Problems - 1
  a. if f(x) = 0 ∀x ∈ Rⁿ ⇒ (OPT) is a feasibility problem
  b. if we have m = 0 constractins ⇒ (OPT) is an unconstrained optimization problem
 
 C. Classification of Problems - 2
  Q: why do we need f and g?
  A: in the absence of hypotheses on f and g, (OPT) is unsolvable
 
  Note: "Black box" optimization framework
  All that is given is an oracle function that can compute values of f(x) for any x 
   (and possibly some extensions to compute derivatives)

 Example: solve 
  min f(x)
  s.t. x ∈ R
  f(x) := 0 when x = λ
  f(x) := 1 otherwise

=LEC2= 2018-09-10
 Example: consider
  min f(x)
  s.t. g(x) ≤ 0, for i ∈ [1, m]
       h(x) ≤ 0
  h(x) when x in Zⁿ, do: 0
  h(x), do: 1

  in other words, we only want integral solutions
 
 defn Discrete Optimization
  When the constraints of (OPT) restrict to a lattice, we have a discrete optimization problem


 defn Continuous
  A function f: D → R is continuous over D 
  if ∀ε > 0, ∃δ > 0 s.t. |x - y| < δ ⇒ |f(x) - f(y)| < ε, ∀x, y ∈ D

 defn Smooth
  A function f: D → R is Cᵏ smooth over D (f ∈ Cᵏ(D)) 
  if all its kth derivatives are continuous over D

 f(x) when x >= 2, do: 1
 f(x), do: -1
  f(x) is discontinous

 g(x), do: abs(x - 2)
  g(x) ∈ C⁰

 h(x) when x >= 2, do: 1/2 (x-2) ** 2
 h(x), do: 1/2 (2-x) ** 2
  h(x) ∈ C¹

 defn Gradient
  Let f ∈ C¹(D) for D ⊆ Rⁿ. The gradient is
   ∇f: D → Rⁿ
   if satisfies ∇f ∈ C⁰(D) and is given by ∇f(x) = (δf/δx_1(x), ..., δf/δx_n(x))~

 defn Hessian
  Let f ∈ C²(D) for D ⊆ Rⁿ. Its Hessian is
   ∇²f: D → Rⁿ
   It satisfies ∇²f ∈ C⁰(D) and is given by ∇²f = 
     | δf(x)/δx_1δx_1 ... δf(x)/δx_nδx_1 |
     | ...            ...            ... |
     | δf(x)/δx_1δx_n ...  δf(x)/δx_nδx_n|

 defn Linear
  A function f: D → R, D ⊆ Rⁿ is linear 
  if ∃c ∈ Rⁿ where f(x) = c~x, ∀x ∈ D
  Then ∇f(x) = c and ∇²f(x) = [0] for all x ∈ D

 remark: if f, g_i are linear, then (OPT) is a linear programming function

_CH1-Linear Algebra_
 A vector and matrix norm

 defn Norm
  A norm ‖.‖ on Rⁿ assigns a scalar ‖x‖ to every x ∈ Rⁿ s.t.
   1) ‖x‖ ≥ 0, ∀x ∈ Rⁿ
   2) ‖cx‖ = |c|‖x‖, ∀c ∈ R, ∀x ∈ Rⁿ
   3) ‖x‖ = 0 ⇔ x = 0 
   4) ‖x + y‖ ≤ ‖x‖ + ‖y‖ ∀x, y ∈ Rⁿ

  Lᵏ norm ‖x‖ₖ = (∑(x_i)ᵏ)^1/k 
   in particular:
    Manhattan Norm: L₁ = ‖x‖₁ = ∑|x_i|
    Euclidean Norm: L₂ = ‖x‖₂ = √(∑x_i²)
    Infinite Norm: L_inf = ‖x‖_inf = max(|x_i|)

  Schwartz inequality: ∀x,y ∈ Rⁿ
   |x~y| ≤ ‖x‖₂ ⋅ ‖y‖₂
   equality when x = λy, for some λ ∈ R

  Pythagorean theorem: If x, y ∈ Rⁿ are orthogonal, then ‖x + y‖₂² = ‖x‖₂² + ‖y‖₂²

 defn Matrix Norm
  Given a vector norm ‖.‖, the induced matrix norm associates a scalar ‖A‖ to all A ∈ Rⁿˣⁿ
   ‖A‖ = max ‖A.x‖, when ‖x‖ = 1

=LEC3= 2018-09-12
  Property of Matrix norm:
   ‖A‖₂ = max ‖A.x‖₂ = max |y~Ax|
         ‖x‖₂ = 1     ‖x‖₂, ‖y‖₂ = 1 
   Proof: Schwartz inequality to |y~Ax|
  Property:
   ‖A‖₂ = ‖A~‖₂
   pf: swap x and y in above property

  Properties: Let A ∈ Rⁿˣⁿ, following are equaivalent
   a) A is non-singular
   b) A~ is non-singular
   c) ∀x ∈ Rⁿ, if x ≠ 0, Ax ≠ 0
   d) ∀b ∈ Rⁿ, ∃x ∈ Rⁿ s.t. Ax = b, and x is unique
   e) the columns of A are linearly independent
   f) the rows of A are linearly independent
   g) ∃B ∈ Rⁿˣⁿ such that AB = I = BA, where B is unique (B is the inverse of A)
   h) for all A, B ∈ Rⁿˣⁿ, (AB)` = B`A` if B` exists

 defn Eigenvalue
  The characteristic polynomial Φ: R → R of A ∈ Rⁿˣⁿ is Φ(λ) = det(A - λI)
  It has n complex roots, the eigenvalues of A
  Given an eigenvalue λ of A, x ∈ Rⁿ is its corresponding eigenvector of A if Ax = λx

  Properties: Given A ∈ Rⁿˣⁿ
  a) λ is an eigenvalue iff ∃ a corresponding eigenvector x
  b) A is singular iff it has a zero eigenvalue
  c) if A is triangular, then its eigenvalues are its diagonal elements
  d) if S ∈ Rⁿˣⁿ is non-singular, and B = SAS`, then A and B have the same eigenvalues
  e) if the eigenvalues of A are λ_1, ..., λ_n (not necessarily distinct)
     - the eigenvalues of A + cI are c+λ_1, ..., c + λ_n
     - the eigenvalues of Aᵏ are λ_1ᵏ, ..., λ_nᵏ
     - the eigenvalues of A` are 1/λ_1, ..., 1/λ_n
     - the eigenvalues of A~ are λ_1, ..., λ_n

 defn Spectral Radius
  the spectral radius ρ(A) of A ∈ Rⁿˣⁿ is the maximum of the magnitudes of its eigenvalues

  Property: 
   For any induced norm ‖.‖, ρ(A) ≤ ‖Aᵏ‖^1/k, for k = 1, 2, ...
   
   pf: 
    By definition, ‖Aᵏ‖ = max ‖Aᵏy‖ = max ‖Aᵏy‖/‖y‖
                         ‖y‖=1       y≠0
    In particular, let λ be any eigenvalue of A, and x its eigenvector
    ‖Aᵏ‖ ≥ ‖Aᵏx‖/‖x‖ = ‖Aᵏ⁻¹Ax‖/‖x‖ = ‖Aᵏ⁻¹λx‖/‖x‖ = ... = ‖λᵏx‖/‖x‖ = (|λᵏ|‖x‖)/‖x‖ = |λᵏ|
    So for any eigenvalue, ‖Aᵏ‖ ≥ |λᵏ| ⇒ ‖Aᵏ‖^1/k ≥ λ ⇒ ρ(A) ≤ ‖Aᵏ‖^1/k, QED

  Property: 
   For any induced norm ‖.‖, lim k→∞ ‖Aᵏ‖^1/k = ρ(A)
   Also, lim k→∞ Aᵏ = 0 iff ρ(A) < 1

   pf: exercise!

 Symmetric Matrices
  Properties: Let A ∈ Rⁿˣⁿ be a symmetric matrix
   a) its eigenvalues are real
   b) its eigenvalues are n mutually orthogonal real nonzero vectors
   c) if the eigenvectors x_1, ..., x_n are normalized s.t. ‖x‖₂ = 1, with corresponding λ_1, ..., λ_n, then A = ∑λ_ix_ix_i~

   pf: exercise!

=LEC4= 2018-09-14
  Property: 
   Let A ∈ Rⁿˣⁿ be a symmetric matrix, then ‖A‖₂ = ρ(A)

   pf: 
    from before, ρ(A) ≤ ‖Aᵏ‖^1\k, in particular, ρ(A) ≤ ‖A^1‖₂^1/1 = ‖A‖₂
    Now, need to prove ρ(A) ≥ ‖A‖₂
    As the eigenvectors xᵢ, i = 1...n of C are mutually orthogonal
    we can write any y ∈ Rⁿ as y = ∑ βᵢxᵢ, i=1...n, for some βᵢ ∈ R

    By Pythagoras' theorem, ‖y‖₂² = ∑ βᵢ² . ‖x‖₂²; i=1...n
    So, Ay = A ∑βᵢxᵢ = ∑βᵢAxᵢ = ∑βᵢλᵢxᵢ
    Again using Pythagoras', ‖Ay‖₂² = ‖∑βᵢλᵢxᵢ‖₂² 
     = ∑βᵢ²λᵢ²‖x‖₂² 
     = ∑|λᵢ|² . |βᵢ|² . ‖x‖₂²
     ≤ ∑ρ(A)²|βᵢ|²‖x‖₂²       # by definition of spectral radius, λᵢ ≤ ρ(A)
     = ρ(A)² ∑|βᵢ|²‖x‖₂²
     = ρ(A)² ‖y‖₂²

    ‖Ay‖₂ ≤ ρ(A) ‖y‖₂
    ⇒ ‖A‖₂ = max ‖Ay‖₂/‖y‖₂ ≤ (ρ(A)‖y‖₂)/‖y‖₂
             y≠0
    ⇒ ‖A‖₂ ≤ ρ(A) 
    QED

  Property:
   Let A ∈ Rⁿˣⁿ be symmetric, with eigenvalues λ₁ ≤ ... ≤ λₙ ∈ R
   Then ∀y ∈ Rⁿ: λ₁‖y‖₂² ≤ y~Ay ≤ λₙ‖y‖₂²

   pf:
    write y as ∑βᵢxᵢ, i=1...n, where βᵢ ∈ R, xᵢ are orthogonal eigenvectors of A
    firstly: 
    y~Ay = (∑βᵢxᵢ)~(∑βᵢλᵢxᵢ) = ∑βᵢ²λᵢ‖xᵢ‖₂²
    wlog, assume ‖xᵢ‖₂ = 1 by normalization, so y~Ay = ∑λᵢβᵢ²

    secondly:
    ‖y‖₂² = ∑βᵢ²‖xᵢ‖² = ∑βᵢ²

    ∑λ₁βᵢ² ≤ ∑λᵢβᵢ² ≤ ∑λₙβᵢ² for any i = 1...n  # notice the subscript of λ
    ⇒ ∑λ₁‖y‖₂² ≤ y~Ay ≤ ∑λₙ‖y‖₂² 
    QED

  Property:
   Let A ∈ Rⁿˣⁿ be symmetric, then ‖Aᵏ‖₂ = ‖A‖₂ᵏ for any natural number k

   pf:
    As A is symmetric, A = A~
    (Aᵏ)~ = (A ... A)ᵏ = A~ ... A~ = A ... A = Aᵏ
    As Aᵏ is symmetric, ‖Aᵏ‖₂ = ρ(Aᵏ)
    The eigenvalues of Aᵏ are λ₁ᵏ, ..., λₙᵏ when λ₁, ..., λₙ are the eigenvalues of A
    So, ρ(Aᵏ) = ρ(A)ᵏ
    as ρ(A) = ‖A‖₂ ⇒ ‖A‖₂ᵏ = ‖Aᵏ‖₂

  Property:
   Let A ∈ Rⁿˣⁿ, then ‖A‖₂² = ‖A~A‖₂ = ‖AA~‖₂

   pf: 
    According to Schwartz inequality: x~y ≤ ‖x‖₂ . ‖y‖₂
    ‖Ax‖₂² = (Ax)~(Ax) = (x~A~)(Ax) = x~ . A~Ax
    ≤ ‖x‖₂ . ‖A~Ax‖₂ 
    ≤ ‖x‖₂ . ‖A~A‖₂ . ‖x‖₂, ∀x ∈ Rⁿ

    Remark:
     ‖A‖₂² = max ‖Ax‖₂²/‖x‖₂² ≤ ‖A~A‖₂
            x∈Rⁿ

=LEC5= 2018-09-17
     ‖A~A‖ = max |y~A~Ax|    
            ‖y‖=1,‖x‖=1
          ≤ max ‖y~A~‖₂ . ‖Ax‖₂
            ‖y‖=1,‖x‖=1
          = (max ‖y~A~‖₂) (max ‖Ax‖₂)
            ‖y‖=1        ‖x‖=1
          = ‖A‖₂²
     So, we have ‖A‖₂² = ‖A~A‖
     For ‖A‖₂² = ‖AA~‖, repeat steps with A and A~ swapped

  Property:
   ‖A`‖₂ is 1/λ₁ where λ₁ is the smallest magnitude eigenvalue of A

   pf:
    Remark that ‖A`‖₂ = ρ(A`), and the eigenvalues of A` are the inverses of the eigenvalues of A
    inverse of smallest = largest

 defn Positive Definite Matrix
  A symmetric matrix A ∈ Rⁿˣⁿ is 
   positive definite if x~Ax > 0 for all x ∈ Rⁿ, x≠0
   positive semidefinite if x~Ax ≥ 0 for all x ∈ Rⁿ

  Property:
   For any A ∈ Rᵐˣⁿ (not necessaily square), A~A is psd, and A~A is pd iff rank(A) = n

   pf:
    A~A is square and symmetric: 
     trivial
    A~A is psd: 
     for any x ∈ Rⁿ, x~(A~A)x = (Ax)~(Ax) = ‖Ax‖₂² ≥ 0
    A~A is pd iff rank(A) = n:
     x~A~Ax > 0 ∀x ∈ Rⁿ, x ≠ 0
     ⇔ ‖Ax‖₂² > 0 ∀x ∈ Rⁿ, x ≠ 0
     ⇔ ‖Ax‖₂ > 0 ∀x ∈ Rⁿ, x ≠ 0
     ⇔ Ax ≠ 0 ∀x ∈ Rⁿ, x ≠ 0
     ⇔ rank(A) = n            by fundamental theory of linear algebra

  Corollary:
   If A ∈ Rⁿˣⁿ is square, A~A is pd iff A is nonsingular

  Properties:
   1. A square symmetric matrix is psd iff all its eigenvalues are ≥ 0 
   2. A square symmetric matrix is pd iff all its eigenvalues are > 0 

   pf (For statement 1):
    Let λ be an eigenvalue of psd matrix A, and let x be its corresponding nonzero eigenvector
    x~Ax ≥ 0, so x~λx = λ‖x‖₂² ≥ 0 
    ⇒ λ ≥ 0

    Let λ₁, ..., λₙ ≥ 0 be the eigenvalues of A, 
     and let x₁, ..., xₙ be the n (nonzero, real, mutually orthogonal) eigenvectors
    As such, ∀y ∈ Rⁿ, y is a linear combination of (x₁, ..., xₙ)
    y = ∑βᵢxᵢ, i=1..n
    y~Ay = (∑βᵢxᵢ)~A(∑βᵢxᵢ)
    = (∑βᵢxᵢ)~(∑βᵢAxᵢ)
    = (∑βᵢxᵢ)~(∑βᵢλᵢxᵢ)
    = ∑βᵢ²λᵢ‖xᵢ‖₂² ≥ 0
    
    For statement 2, do it yourself

  Property:
   The inverse of a pd matrix is also pd
   
   pf:
    Let λ₁, ..., λₙ > 0 be the eigenvalues of A, then clearly their inverses (the eigenvalues of A`) must also be positive

_CH2-Convexity_
 defn Convex
  A set C ⊆ Rⁿ is convex if 
   λx + (1 - λy) ∈ C, ∀x,y ∈ C, ∀λ ≤ 1, λ ≥ 0

=LEC6= 2018-09-19
  A function f: D → R is convex if
   f(λx + (1 - λ)y) ≤ λf(x) + (1 - λ)f(y) ∀x,y ∈ D, ∀λ ≤ 1, λ ≥ 0
  f is strictly convex if strict inequality holds
   f(λx + (1 - λ)y) < λf(x) + (1 - λ)f(y) ∀x,y ∈ D, ∀λ ≤ 1, λ ≥ 0

  Properties:
   - for any collection {cᵢ : i ∈ I} of convex sets, their intersection ∩cᵢ is convex
   - the vector (Minkowski) sum {x + y : x ∈ C₁, y ∈ C₂} of convex sets C₁, C₂ is convex
   - the image of a convex set under a linear transformation is convex

 defn Level Set
  Let f: C → R be a function, with a convex domain C
  the level sets of f are {x ∈ C: f(x) ≤ α} ∀α ∈ R

 defn Epigraph
  f as before
  the epigraph of f is a subset of Rⁿ⁺¹ given by epi(f) = {(x, α) : x ∈ C, α ∈ R, f(x) ≤ α}

  Properties
   a. if f: C → R is convex, then its level sets are convex
    The converse of a is not true
     the level sets of f(x) = √|a| are {x: -α² ≤ x ≤ α²}
     however, f(x) is not convex: let x = 0, y = 1, λ = 0.5
     f(0.5) = √0.5, 0.5f(0) + 0.5f(1) = 0.5
     √0.5 > 0.5!
   b. f: C → R is convex iff its epigraph is a convex set
   c. any linear function is convex (not necessarily strictly)
   d. if f is a convex function, g(x) = λf(x) is convex ∀λ ≥ 0
   e. the sum of two convex functions is a convex function
   f. the max of two convex functions is a convex (piecewise) function
   g. Any vector norm is convex
    pf:
     Let f(x) = ‖x‖, then for any x,y ∈ Rⁿ, 0 ≤ λ ≤ 1:
      f(λx + (1 - λ)y) 
      = ‖λx + (1 - λ)y‖ 
      ≤ ‖λx‖ + ‖(1 - λ)y‖      # by definition of a norm
      = λ‖x‖ + (1 - λ)‖y‖ 
      = λf(x) + (1 - λ)f(y)

=LEC7= 2018-09-21
 Taylor's theorem for univariate functions
  f(x + h) = ∑[hⁱ/i! dᵢ(f(x))] + ϕ(h) for i = 0..k, where dᵢ(f) is the ith derivative of f
  Φ(h) = hᵏ⁺¹/(k + 1)! dₖ₊₁(f(x + λh)), 0 ≤ λ ≤ 1 is the residual function
  In particular, lim h→0 Φ(h)/hᵏ = 0

 Taylor's theorem for multivariate functions
  1st order (k = 1)
   f(x + h) = f(x) + h~∇f(x) + Φ(h)
   Φ(h) = 1/2 h~∇²f(x + λh)h, 0 ≤ λ ≤ 1
   with lim h→0 Φ(h)/‖h‖ = 0

  2nd order
   f(x + h) = f(x) + h~∇f(x) + 1/2h~∇²f(x)h + Φ(h)
   with lim h→0 Φ(h)/‖h‖² = 0

 Mean Value theorem
  Let f: D → R, D ⊆ R, f ∈ C¹(D)
  then ∀x,y ∈ D, ∃z ∈ [x, y] f(y) = f(x) + ∇f(z)(y-x),
   pf: by zeroth order Taylor expansion

 defn Directional Derivative
  The directional derivative of f in the direction of y is 
   ∇_y f(x) = lim α→0 (f(x + αy) - f(x))/α

  In particular, ∇_eᵢ f(x) = δf/δxᵢ(x) and ∇f = (∇_e₁f(x) ... ∇_eₙf(x))~
  "direction" -> draw out the function!

 Thm:
  If f ∈ C¹, then ∇_hf = h~∇f

  pf:
   ∇_hf 
   = lim α→0 (f(x + αh) - f(x))/α 
   = lim α→0 (f(x) + αh~∇f(x) + Φ(αh) - f(x)) / α
   = lim α→0 (αh~∇f(x) + Φ(αh)) / α
   = lim α→0 αh~∇f(x) / α + lim α→0 Φ(αh) / α
   = h~∇f(x) + lim α→0 Φ(αh) / α  # see definition of residual above
   = h~∇f(x)

  Property:
   Let C ⊆ Rⁿ be convex, and let f: C → R be differentiable over C.
    f is convex iff
     f(z) ≥ f(x) + (z-x)~∇f(x) ∀x,z ∈ C
    pf:
     (->) As C is convex, x + (z-x)α = αz + (1-α)x ∈ C, ∀0 ≤ α ≤ 1
      lim→0 (f(x+α(z-x)) - f(x))/α = ∇_z-x f(x) = (z-x)~∇f(x)

      By convexity of f, f(x+α(z-x)) ≤ αf(z) + (1-α)f(x), ∀0 ≤ α ≤ 1
      f(x+α(z-x)) - f(x) ≤ αf(z) - αf(x)
      (f(x+α(z-x)) - f(x))/α ≤ f(z) - f(x)
      taking lim α→0:
      (z-x)~∇f(x) ≤ f(z) - f(x)

=LEC8= 2018-09-24
     (<-) if f(z) ≥ f(x) + (z-x)~∇f(x), ∀x,z ∈ C
     Let a,b ∈ C be any points in the domain of f, let c = αa + (1 - α)b
      (1) f(a) ≥ f(c) + (a - c)~∇f(c)
      (2) f(b) ≥ f(c) + (b - c)~∇f(c)

     multiply (1) by α and (2) by (1 - α), then add them:
      αf(a) + (1 - α)f(b) ≥ α(f(c) + (a - c)~∇f(c)) + (1 - α)(f(c) + (b - c)~∇f(c))
      αf(a) + (1 - α)f(b) ≥ f(c) + α((a - c)~∇f(c)) + (1 - α)((b - c)~∇f(c))
      αf(a) + (1 - α)f(b) ≥ f(c) + (αa - αc + b - αb - c + αc) ~∇f(c))
      αf(a) + (1 - α)f(b) ≥ f(c) + (αa + b - αb - c) ~∇f(c)) # by definition, c = αa + (1 - α)b
      αf(a) + (1 - α)f(b) ≥ f(c)
      αf(a) + (1 - α)f(b) ≥ f(αa + (1 - α)b)
     Therefore, f is convex over C.
  
  Properties: Let f: Rⁿ → R, f ∈ C²(D)
   a. ∇²f(x) psd ∀x ∈ D ⇒ f convex over D
    pf:
     for all x, y ∈ D, by 1st order Taylor
     f(y) = f(x) + (y - x)~∇f(x) + 1/2 (y-x)~∇²f(x + α(y-x))(y-x) for some 0 ≤ α ≤ 1
   b. ∇²f(x) pd ∀x ∈ D ⇒ f strictly convex over D
    pf:
     similar to a, with y ≠ x, strict inequality.
   c. D = Rⁿ and f convex over D ⇒ ∇²f(x) psd ∀x ∈ D
    pf:
     f is convex over Rⁿ. Assume by contradiction, ∃x,z ∈ Rⁿ where z~∇²f(x) < 0
     because ∇²f is continuous, can find a z small enough that 
      z~∇²f(x + αz)z < 0 ∀0 ≤ α ≤ 1 (1)
     By Taylor:
      f(x + z) = f(x) + z~∇f(x) + 1/2z~∇²f(x + βz)z for some 0 ≤ β ≤ 1
               < f(x) + z~∇f(x)       #by (1)
     Which contradicts convexity

_CH3-Optimality Conditions_
 Thm: [First order necessary conditions for optimality]
  Let f: Rⁿ → R be C¹-smooth
  If x* is a local minimizer, then ∇f(x*) = 0

  pf:
   let B_δ(x*) be such that f(x*) ≤ f(x) ∀x ∈ B_δ(x*)
   ∀i, ∀-δ < h < δ, f(x* + h.eᵢ) - f(x*) ≥ 0
   So, (f(x* + h.eᵢ) - f(x*))/h ≥ 0 if h > 0
       "                     " ≤ 0 if h < 0
   Because f ∈ C¹ lim h → 0 (f(x* + h.eᵢ) - f(x*))/h exists
   If its both >= 0 and <= 0, so = 0
   δf/δxᵢ(x*) = 0 ∀i, so ∇f(x*) = 0

 defn Critical/Stationary
  All x such that ∇f(x) = 0 are called critical or stationary points
  All local minimizers are critical points, but the converse is not true.

=LEC9= 2018-09-26
 Note: δ²f/δxᵢδxⱼ = δ²f/δxⱼδxᵢ, so ∇²f is symmetric

 Thm: [Second Order necessary conditions for optimality]
  Let f: Rⁿ → R be C²-smooth
  If x* is a local minimizer, then ∇f(x*) = 0, ∇²f(x*) is psd

  pf:
   Let z ∈ Rⁿ\{0}, need to prove z~∇²f(x*)z ≥ 0
   Let B_δ(x*) be f(x*) ≤ f(x), ∀x ∈ B_δ(x*)        # the ball
   Let y = hz/‖z‖ with 0 < h < δ

   f(x* + y) - f(x*) ≥ 0
   f(x*) + y~∇f(x*) + 1/2 y~∇²f(x*)y + ϕ(y) - f(x*) ≥ 0 where lim y → 0 ϕ(y)/‖y‖² = 0, y ≠ 0
   by 1st order condition: y~∇f(x*) = 0
   1/2 y~∇²f(x*)y + ϕ(y) ≥ 0
   1/2 h²/‖z‖² z~∇²f(x*)z + ϕ(hz/‖z‖) ≥ 0
   z~∇²f(x*)z + 2‖z‖²/h² ϕ(hz/‖z‖) ≥ 0
   z~∇²f(x*)z + 2‖z‖² lim h → 0 ϕ(hz/‖z‖)/h² ≥ 0
   z~∇²f(x*)z ≥ 0 ∀z QED
 
 Thm: [Second Order sufficient conditions for local optimality]
  Let f: Rⁿ → R, with f ∈ C²(B_δ(x*)). x* ∈ Rⁿ, δ > 0
  If ∇f(x*) = 0 and ∇²f(x*) is pd, then x* is a strict local minimizer

  pf:
   By Taylor 2-order: ∀h ∈ B_δ(x*), f(x* + h) = f(x*) + h~∇f(x*) + 1/2h~∇²f(x*)h + ϕ(h)
   with lim h→0 ϕ(h)/‖h‖² = 0, h ≠ 0

   Let 0 < λ₁ < ... < λₙ be the eigenvalues of ∇²f(x*)
   ∃r > 0: ∀h∈B_r(x*), |ϕ(h)/‖h‖²| ≤ λ₁/4
   ⇒ |ϕ(h)| ≤ ‖h‖²λ₁/4

   Notice that ‖y‖²λ₁ ≤ y~∇²f(x*)y ≤ ‖y‖²λ₂
   f(x*+h) = f(x*) + 1/2h~∇²f(x*)h + ϕ(h)
           ≥ f(x*) + 1/2‖h‖²λ₁ - 1/4‖h‖²λ₁ 
           = f(x*) + 1/4‖h‖²λ₁             # ‖h‖² > 0, λ₁ > 0
           > f(x*) ∀ h ∈ B_r(x*)
   so x* is a strict local minimizer over B_r(x*), as required. QED

 Summary of necessary + sufficient optimality conditions
  a. ∇f(x*) = 0, ∇²f(x*) pd ⇒ x* is a strict local minimizer
  b. x* is a strict local minimizer ⇒ x* is a local minimizer
  c. x* is a local minimizer ⇒ ∇f(x*) = 0, ∇²f(x*) psd
  
  The converses of each statement are untrue!
   a. f(x) = x⁴, x* = 0, hessian not pd
   b. f(x) = 1, x* = 0, obviously not strict!
   c. f(x) = x³, x* = 0, obviously not minimizer